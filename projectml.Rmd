---
title: "Practical Machine Learinng: Human Activity Recognition"
output: html_document
---
###  Summary
In this project, our goal is to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants in order to  quantify how well the excerises are done. 

Our aim  is to predict the manner in which they did the exercise. This is the "classe" variable in the training set. We will provide the following:

* Prediction model 
* Cross validation on the data
* The expected out of sample error
* Predict 20 different test cases using our prediction model

We will use random forest algorithm. For cross-validaiton we will use k-fold algorithm with different values of k. Finally we present our prediction on the given test data.

### The Input Data: Preprocessing
We first clean our dataset by removing culomns with many NAs: more than 97 percent of the number of rows. We also get rid of the features that are not reasonable to be included in our prediction model (the first seven columns. )

```{r, warning=FALSE}
file_dest_training <- "pml-training.csv"
file_dest_testing <- "pml-testing.csv"

# Import the data treating empty values as NA.
df_training <- read.csv(file_dest_training, na.strings=c("NA","", "#DIV/0!"), header=TRUE)
df_testing <- read.csv(file_dest_testing, na.strings=c("NA","", "#DIV/0!"), header=TRUE)
colnames_train <- colnames(df_training)
colnames_test <- colnames(df_testing)
# Verify that the column names (excluding classe and problem_id) are identical in the training and test set.
all.equal(colnames_train[1:length(colnames_train)-1], colnames_test[1:length(colnames_train)-1])
# Count the number of NAs in each col.
areNAs <- function(x) {
    as.vector(apply(x, 2, function(x) length(which(is.na(x)))))
}

# Build vector of missing data or NA columns to drop.
colcnts <- areNAs(df_training)
drops <- c()
for (cnt in 1:length(colcnts)) {
    if (colcnts[cnt] > 0.97*nrow(df_training)) {
        drops <- c(drops, colnames_train[cnt])
    }
}

# Drop NA data and the first 7 columns as they're unnecessary for predicting.
df_training <- df_training[,!(names(df_training) %in% drops)]
df_training <- df_training[,8:length(colnames(df_training))]

df_testing <- df_testing[,!(names(df_testing) %in% drops)]
df_testing <- df_testing[,8:length(colnames(df_testing))]

# Show remaining columns.
colnames(df_training)

```

We next check for features with no variation in their values. After using the function below we observe that all variables have variation. Therefore, there is no need to eliminate any feature for this reason.

```{r, message=FALSE}
library (caret)
nearzv <- nearZeroVar(df_training, saveMetrics=TRUE)
```

We now create the training and validation data partitions on our training data:
```{r}
set.seed(10)
inTrain <- createDataPartition(y=df_training$classe, p=0.7, list=F)
df_train_use <- df_training[inTrain, ]
df_validation_use <- df_training[-inTrain, ]
```

### Building the Model and Cross-Validation
We will use random forest model. We also use different k values in k-fold cross-validation and then we choose the k with best accuracy. Notice that in order to train our model we use the training subset of the provided training dataset. We the compute our estimated out of sample error on the validation dataset. 

```{r, message=FALSE, warning=FALSE}
# instruct train to use k-fold CV to select optimal tuning parameters
i = 1
accuracies <- c(3:4)
for(k in 3:4){
  fitControl <- trainControl(method="cv", number=k, verboseIter=F)
  # fit model 
  fit <- train(classe ~ ., data=df_train_use, method="rf", trControl=fitControl)  
  preds <- predict(fit, newdata=df_validation_use)

  # show confusion matrix to get estimate of out-of-sample error
  confmatrix<-confusionMatrix(df_validation_use$classe, preds)
  accuracies[i] <- confmatrix$overall[1]
  i = i+1
  i
}

bestk_fold <- which.max(accuracies)
fitControl <- trainControl(method="cv", number=bestk_fold, verboseIter=F)
fit <- train(classe ~ ., data=df_train_use, method="rf", trControl=fitControl)  
preds <- predict(fit, newdata=df_validation_use)

# show confusion matrix to get estimate of out-of-sample error
confmatrix<-confusionMatrix(df_validation_use$classe, preds)
confmatrix
```

Since the accuracy is 99.3 percent we predict that our out-of-sample is 0.7 percent.
We next re-train our model on the full training dataset:

```{r}
fitControl <- trainControl(method="cv", number=bestk_fold, verboseIter=F)
fit <- train(classe ~ ., data=df_training, method="rf", trControl=fitControl)  
```

### Prediction on Test Dataset
We next perform the prediction on our test data with 20 elements:

```{r}
preds <- predict(fit, newdata=df_testing)
preds <- as.character(preds)
preds

```
